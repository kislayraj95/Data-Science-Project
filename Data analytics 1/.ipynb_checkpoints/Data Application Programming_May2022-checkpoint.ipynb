{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74ba7504-eae2-4711-bfcb-a2efefd61265"
   },
   "source": [
    "# Data Application Programming\n",
    "\n",
    "This exercise will involve working with a Spark RDD dataset of forum posts.\n",
    "\n",
    "Each forum post has various attributes, such as the author, URL of the post, the title of the post, number of points the post have, when the post was created, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1027dfc-8149-45db-bf36-ce3b5a9f97c8"
   },
   "source": [
    "# Setup\n",
    "\n",
    "You will need pyspark installed on your machine.\n",
    "\n",
    "You can then run the lines below to start a new Spark context and load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8412a9fd-bfae-4d76-b194-18f025657245"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8049cd17ce0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "print(sc)\n",
    "print(\"Ready to go!\")\n",
    "\n",
    "dataset_json = sc.textFile(\"data/HNStories.json\")\n",
    "dataset = dataset_json.map(lambda x: json.loads(x))\n",
    "dataset.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4e00dfc-19fb-4662-bc34-2c36e43c8721"
   },
   "source": [
    "## Q1 [5 marks]\n",
    "## Counting elements in a Spark dataset\n",
    "\n",
    "Write a function which returns the number of elements in a dataset loaded on Spark. \n",
    "\n",
    "This function should take one parameter named `dataset`, of type Spark RDD.\n",
    "\n",
    "It should return the number of elements in the Spark RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39a9b666-99af-4436-9949-29bb582a8b8e"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc177a4d-6ea7-4284-aeef-4483a31f5127"
   },
   "source": [
    "## Q2 [5 marks]\n",
    "## Getting the first element\n",
    "\n",
    "Write a function which returns the first element of a dataset loaded on Spark.\n",
    "\n",
    "This function should take one parameter named `dataset`, of type Spark RDD.\n",
    "\n",
    "It should return the first element of the Spark RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3a3af529-241e-49ca-acdc-8184410df29a"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "266fd8dd-b451-48b8-a770-ff21c521e7ba"
   },
   "source": [
    "## Q3 [10 marks]\n",
    "## Getting all the attributes of an element\n",
    "\n",
    "Each element of a Spark RDD is a dictionary of key:value pairs.\n",
    "\n",
    "Write a function which finds all of the *unique* attributes used throughout a Spark RDD.\n",
    "\n",
    "This function should take one parameter named `dataset`, of type Spark RDD.\n",
    "\n",
    "It should return a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0950d2be-89f3-4b02-b31d-0d6385eabc38"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99150b9c-6a3b-4812-a592-b92cf28bb034"
   },
   "source": [
    "## Q4 [10 marks]\n",
    "## Finding the earliest timestamp of a dataset - 10 marks\n",
    "\n",
    "The machine-readable timestamp of an element of a Spark RDD is stored as the attribute `created_at_i`.\n",
    "\n",
    "Write a function which finds the minimum (i.e. the earliest) timestamp of a Spark RDD.\n",
    "\n",
    "This function should take one parameter named `dataset`, of type Spark RDD.\n",
    "\n",
    "It should return a more human-readable`datetime` object.\n",
    "\n",
    "(A helper function is provided for converting the Spark RDD timestamps to a `datetime` object.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2c63cba4-2815-496e-bb44-3e3ae95e210f"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def extract_time(timestamp):\n",
    "    return dt.utcfromtimestamp(timestamp)\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa68bf9b-8c62-4f24-8d6c-c795db27a12b"
   },
   "source": [
    "## Q5 [20 marks]\n",
    "## Calculating the proportion of successful posts per author - 20 marks\n",
    "\n",
    "A post is successful if it has strictly more than 200 points.\n",
    "\n",
    "Write a function to find the proportion of posts in the Spark RDD dataset which are successful, for each author in the dataset.\n",
    "\n",
    "This function should take one parameter named `dataset`, of type Spark RDD.\n",
    "\n",
    "It should return a Spark RDD showing the proportion of successful posts per author.\n",
    "\n",
    "Note: If an entry in the dataset has no value for author, use the string `unknown` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fea2180-aa47-4d0b-bce7-2097de0047cf"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data Application Programming 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
